{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DRAM I-V Prediction using Neural Network\n",
        "\n",
        "DRAM 소자의 I-V 특성을 예측하는 신경망 모델\n",
        "\n",
        "**데이터**: 1718개 샘플, 5개 특성  \n",
        "**입력**: Spacer(nm), Doping(cm⁻³), Gate(nm), Voltage(V)  \n",
        "**출력**: Current(A)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 시드 설정\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# 디바이스\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {device}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 데이터 로딩\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 데이터 로딩\n",
        "data = np.load('dram_data.npy')\n",
        "\n",
        "print(f\"Shape: {data.shape}\")\n",
        "print(f\"\\n칼럼:\")\n",
        "print(f\"  [0] spacer_length (nm)\")\n",
        "print(f\"  [1] doping_conc (cm⁻³)\")\n",
        "print(f\"  [2] gate_length (nm)\")\n",
        "print(f\"  [3] voltage (V)\")\n",
        "print(f\"  [4] current (A)\")\n",
        "\n",
        "# 처음 5개 확인\n",
        "print(f\"\\n처음 5개 샘플:\")\n",
        "print(data[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 데이터 전처리\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Signed Log 변환 함수\n",
        "def signed_log(x, eps=1e-60):\n",
        "    return np.sign(x) * np.log10(np.abs(x) + eps)\n",
        "\n",
        "def inverse_signed_log(y, eps=1e-60):\n",
        "    return np.sign(y) * (10 ** np.abs(y) - eps)\n",
        "\n",
        "# 입력/출력 분리\n",
        "X = data[:, :4].copy()  # spacer, doping, gate, voltage\n",
        "y = data[:, 4:5].copy()  # current\n",
        "\n",
        "# 변환\n",
        "X[:, 1] = np.log10(X[:, 1])  # doping → log10\n",
        "y = signed_log(y)             # current → signed log\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"\\n변환 후 범위:\")\n",
        "print(f\"  Spacer: [{X[:, 0].min():.1f}, {X[:, 0].max():.1f}]\")\n",
        "print(f\"  Doping(log): [{X[:, 1].min():.1f}, {X[:, 1].max():.1f}]\")\n",
        "print(f\"  Gate: [{X[:, 2].min():.1f}, {X[:, 2].max():.1f}]\")\n",
        "print(f\"  Voltage: [{X[:, 3].min():.1f}, {X[:, 3].max():.1f}]\")\n",
        "print(f\"  Current(log): [{y.min():.1f}, {y.max():.1f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 정규화\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "# Train/Val/Test 분할\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_scaled, y_scaled, test_size=0.15, random_state=SEED)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15/0.85, random_state=SEED)\n",
        "\n",
        "print(f\"Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"Val:   {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\n",
        "print(f\"Test:  {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# PyTorch 텐서 변환\n",
        "X_train_t = torch.FloatTensor(X_train).to(device)\n",
        "y_train_t = torch.FloatTensor(y_train).to(device)\n",
        "X_val_t = torch.FloatTensor(X_val).to(device)\n",
        "y_val_t = torch.FloatTensor(y_val).to(device)\n",
        "X_test_t = torch.FloatTensor(X_test).to(device)\n",
        "y_test_t = torch.FloatTensor(y_test).to(device)\n",
        "\n",
        "# DataLoader\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Train batches: {len(train_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ㄹ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "class DRAM_NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(4, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "model = DRAM_NN().to(device)\n",
        "print(model)\n",
        "print(f\"\\n파라미터: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
        "\n",
        "NUM_EPOCHS = 200\n",
        "best_val_loss = float('inf')\n",
        "patience = 0\n",
        "EARLY_STOP = 30\n",
        "\n",
        "history = {'train_loss': [], 'val_loss': [], 'lr': []}\n",
        "\n",
        "print(\"학습 시작...\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        pred = model(X_batch)\n",
        "        loss = criterion(pred, y_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * len(X_batch)\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    \n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_pred = model(X_val_t)\n",
        "        val_loss = criterion(val_pred, y_val_t).item()\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "    \n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1:3d} | Train: {train_loss:.6f} | Val: {val_loss:.6f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        patience = 0\n",
        "    else:\n",
        "        patience += 1\n",
        "    \n",
        "    if patience >= EARLY_STOP:\n",
        "        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print(f\"\\n✅ 학습 완료! Best val loss: {best_val_loss:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# 학습 곡선\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "axes[0].plot(history['train_loss'], label='Train')\n",
        "axes[0].plot(history['val_loss'], label='Val')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Loss Curve')\n",
        "axes[0].set_yscale('log')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(history['lr'], color='red')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Learning Rate')\n",
        "axes[1].set_title('LR Schedule')\n",
        "axes[1].set_yscale('log')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
